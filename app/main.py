from __future__ import annotations
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from langgraph.checkpoint.sqlite import SqliteSaver

from .schema import ChatRequest, ChatResponse
from .graph import build_graph
from .config import settings

# We'll create both graphs during app startup and keep the SQLite context open
graphs = {}

# ---------- Lifespan (startup/shutdown) ----------

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Temporary graph (RAM). No checkpointer passed -> build_graph uses MemorySaver().
    graphs["temporary"] = build_graph()

    # Persistent graph (SQLite) â€” OPEN the context manager and keep it alive
    with SqliteSaver.from_conn_string(settings.sqlite_path) as sqlite_saver:
        graphs["persistent"] = build_graph(checkpointer=sqlite_saver)
        # What you put before 'yield' it executes during startup
        yield
        # context closes automatically on shutdown

# ---------- App ----------

# Attach the lifespan so our graphs are created/cleaned safely.
app = FastAPI(title="Chatbot Backend (LangChain+LangGraph)", lifespan=lifespan)

# ---------- Routes ----------

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """
    Chat endpoint:
      - Selects the graph by memory mode ('temporary' or 'persistent').
      - Uses `session_id` as `thread_id` to load/save conversation state via the checkpointer.
      - Invokes the graph with the current user message.
      - Returns the assistant's last reply.
    """
    mode = req.memory
    if mode not in graphs:
        raise HTTPException(status_code=400, detail="memory must be 'temporary' or 'persistent'")

    # Pass the conversation identifier so the checkpointer can persist/recover state.
    # `thread_id` is how LangGraph ties turns together across requests.
    config = {"configurable": {"thread_id": req.session_id}}
    # Invoke the graph:
    # - Input state includes the latest user_input and an empty messages list.
    #   The checkpointer will hydrate prior history for this thread_id.
    result = graphs[mode].invoke({"user_input": req.message, "messages": []}, config=config)

    # result is the last ChatState that the graph returned.
    messages = result.get("messages", []) # obtain the message list (history) if there is nothing return empty list
    reply = messages[-1].content if messages else "" # access to the last message (AIMessage generated by LLM)

    # Shape the HTTP response using our Pydantic response model.
    return JSONResponse(
        ChatResponse(
            session_id=req.session_id,
            reply=reply,
            mode=mode,
            tokens_input=None,
            tokens_output=None,
        ).model_dump() # convert pydantic model into python dict to serialize JSON
    )